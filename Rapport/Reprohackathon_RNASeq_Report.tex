\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{hyperref} % For clickable links
\usepackage{url}
\usepackage{listings}
\usepackage{xcolor}
\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{float}

\title{Reprohackathon-RNASeq}
\author{Donatien Wallaert, Tom Bellivier, Marie Meier, Tom Gortana}
\date{November 2025}

\begin{document}

\maketitle

\section{Sommaire}

\begin{enumerate}
  \item Introduction
  \item Material and Methods (Tools used, and setup)
  \item Results (Workflow developed, results obtained after execution)
  \item Conclusion / perspectives (Interpretation of the results, and conclusion about reproducibility)
\end{enumerate}

\section{Introduction}

Reproducibility is a core principle of the scientific method, as it allows independent researchers to validate findings and extend previous work. In bioinformatics and computational biology, achieving reproducible analyses is particularly challenging due to complex software stacks, evolving reference data, and often incomplete documentation. Addressing these issues requires integrated frameworks that combine version control, containerization, workflow management, and clear reporting of analytical steps.

This reprohackathon project focuses on reproducing the RNA-sequencing analysis from Peyrusson et al. (Nature Communications, 2020), which examined intracellular Staphylococcus aureus persisters during antibiotic exposure. Persister cells are phenotypic variants that transiently tolerate antibiotics without carrying genetic resistance mutations, making them important contributors to treatment failure and recurrent infections.

\subsection{Biological Context: Staphylococcus aureus Persisters and Antibiotic Tolerance}

Staphylococcus aureus is a major human pathogen causing infections that range from mild skin disease to severe invasive syndromes. Intracellular survival of S. aureus within host cells such as macrophages is thought to promote infection recurrence by sheltering bacteria from immune responses and antibiotic activity. Under intracellular stresses, a subpopulation can adopt a persister state characterized by growth arrest and high tolerance to bactericidal antibiotics.

Peyrusson et al. used single-cell fluorescence-based approaches to show that intracellular S. aureus surviving antibiotic treatment display biphasic killing kinetics, with most bacteria rapidly dying and a minority persisting for prolonged periods. The persister phenotype was stable under antibiotic pressure but reversible once the drug was removed, as non-dividing cells resumed growth when the stress was lifted.

Transcriptomic profiling in the same study revealed that intracellular persisters remained metabolically active but exhibited a markedly reprogrammed gene expression pattern. RNA-seq identified 1,477 differentially expressed genes between intracellular persisters and control bacteria, including induction of stringent response, cell wall stress, SOS DNA damage, and heat shock pathways, which together were associated with multidrug tolerance and cross-tolerance to unrelated antibiotics.

\subsection{RNA-Sequencing and Differential Gene Expression Analysis}

RNA sequencing (RNA-seq) enables genome-wide, quantitative measurement of transcript abundance with a broad dynamic range and the ability to detect novel transcripts compared with microarrays. Typical RNA-seq analysis pipelines include quality control, read trimming, alignment to a reference genome, quantification of reads per gene, and statistical testing for differential expression. Differential expression tools such as DESeq2 and edgeR model count data using negative binomial distributions, whereas limma applies linear models to transformed expression values.

\subsection{Reproducibility Challenges in RNA-Seq Analysis}

RNA-seq results can vary substantially across pipelines due to differences in software versions, reference annotations, parameter choices, and statistical methods, which may alter the set of genes reported as differentially expressed. Even when using the same raw data, small changes in tools or settings can propagate to biologically relevant discrepancies in downstream interpretation. To mitigate this, current best practices emphasize tracking code with Git, encapsulating environments using Docker or similar containers, and orchestrating workflows with systems such as Nextflow or Snakemake to ensure that analyses remain executable and auditable over time.

\subsection{The Reprohackathon Approach}

Reprohackathons are training-oriented hackathon events where participants attempt to reproduce published analyses using modern reproducibility technologies. These projects expose students to real-world obstacles such as missing methods, unavailable data, deprecated software, and limited computational resources, while also demonstrating how containerization and workflow systems can overcome such barriers.

In this project, the reprohackathon centers on reproducing the RNA-seq component of the Peyrusson et al. study, focusing on differential gene expression between intracellular S. aureus persisters exposed to oxacillin and control bacteria. The workflow relies on Docker for environment management, Nextflow for pipeline automation, and Git for systematic tracking of code and configuration changes, with the goal of producing a fully reproducible and shareable analysis.

\section{Materials and Methods}

\subsection{Study Design and Data Sources}
The original study deposited RNA-seq data in the Gene Expression Omnibus (GEO) under accession number GSE139659 (\url{https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139659}). The experimental design compared intracellular persisters---bacteria recovered from J774 macrophages exposed to 50$\times$ MIC oxacillin for 24 hours---against control samples consisting of bacteria mixed with cell lysate from non-infected macrophages.

The original experimental protocol involved infecting J774 macrophages with \textit{S. aureus} strain SH1000 expressing GFP from a tetracycline-inducible promoter at a multiplicity of infection of 100:1. Following phagocytosis, cells were exposed to high concentrations of oxacillin to induce a homogeneous population of persisters. Intact, viable persisters were isolated by fluorescence-activated cell sorting (FACS) by gating GFP-positive, propidium iodide-negative events, ensuring that only live, intracellular bacteria were collected for RNA extraction. Three biological replicates were generated for both persister and control conditions.

Figure~\ref{fig:Workflow} summarizes the complete computational workflow implemented to reproduce the RNA-seq analysis, from data download to statistical analysis in DESeq2.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{process_docker.png}
    \caption{Complete Workflow}
    \label{fig:Workflow}
\end{figure}

\subsection{FASTQ File Download: Project-wide and Single-run Strategies}
The initial step of our RNA-seq workflow involved the automated retrieval of raw sequencing data in FASTQ format for each SRA run accession associated with the study. To ensure computational reproducibility and ease of deployment across different environments, the download process was encapsulated both as a dedicated Nextflow process and within a custom Docker container built for the SRA Toolkit.

\subsubsection{Unified Download Logic}
All downloads are performed via the \texttt{DOWNLOAD\_FASTQ} Nextflow process, which encapsulates the SRA Toolkit in a dedicated Docker container. This guarantees that every file, regardless of how it is selected, is downloaded in the same controlled, reproducible environment. The process not only downloads each FASTQ file, but also automatically compresses it (\texttt{.gz}) for efficient storage and downstream processing. Additionally, in ``test mode," it can produce a truncated file containing only the first 10{,}000 reads for workflow validation.

\textbf{Key script logic:}
\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
echo "Downloading FASTQ for ${sra_id}"
prefetch ${sra_id}
if [ "${params.test}" == "true" ]; then
    # Test mode: keep first 10,000 reads
    fasterq-dump --threads ${task.cpus} -t . --progress ${sra_id}
    gzip ${sra_id}.fastq
    zcat ${sra_id}.fastq.gz | head -n 10000 | gzip > ${sra_id}_test.fastq.gz
    mv ${sra_id}_test.fastq.gz ${sra_id}.fastq.gz
else
    fasterq-dump --threads ${task.cpus} -t . --progress ${sra_id}
    gzip ${sra_id}.fastq
fi
echo "DOWNLOADING completed: ${sra_id}"
\end{lstlisting}

This code downloads, compresses, and (if in test mode) truncates each FASTQ file.

\subsubsection{Two Modes of SRR Retrieval}

\paragraph{Download All Samples via SRA Project Accession:}
When the SRA project accession (e.g., SRP227811) is given:
The \texttt{GET\_SRR} process uses a Docker image with Entrez Direct installed to query the NCBI SRA database and retrieve all associated SRR numbers.

Example script:
\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
esearch -db sra -query ${sra_project} | efetch -format runinfo | cut -d',' -f1 | grep SRR > SRR_list.txt
\end{lstlisting}

This extracts every SRR accession for the given project. The resulting list (e.g., six files: 3 persister, 3 control) is passed to the \texttt{DOWNLOAD\_FASTQ} process.
Each SRR is then downloaded using the same SRA Toolkit Docker, ensuring environment and method consistency.

Note: Entrez Direct Docker provides only the utilities (\texttt{esearch}, \texttt{efetch}, etc.) to extract all run accessions for a given project. It is not used for downloading FASTQ files themselves.

\paragraph{Download an Individual File via SRR:}
When a specific SRR number is provided (e.g., SRR10379726), the workflow directly invokes the \texttt{DOWNLOAD\_FASTQ} process for that accession, using the SRA Toolkit Docker container.

\subsubsection{Workflow Integration and Reproducibility}
By separating SRR list generation (project context) from data download, and by using \texttt{DOWNLOAD\_FASTQ} exclusively for all file retrieval, the workflow achieves:
\begin{itemize}
    \item \textbf{Complete reproducibility} of the download operation through a single well-defined environment (SRA Toolkit Docker).
    \item \textbf{Flexibility} for users: download a whole project dataset (using SRA project accession + \texttt{GET\_SRR}) or a single file (using one SRR).
    \item \textbf{Robustness and transparency}: only a single, well-audited process (\texttt{DOWNLOAD\_FASTQ}) performs the key task of file downloading, with full provenance and containerization benefits for traceability, auditing, and scientific rigor.
\end{itemize}

\subsection{Read Trimming: Trim Galore and Cutadapt}
During this step, raw sequencing reads are processed to remove low-quality bases and adapter contamination before mapping. To mirror the pipeline from the original publication, we used Trim Galore version 0.4.4 and Cutadapt version 1.11, both packaged in a custom Docker image based on Ubuntu 18.04 and Python 3.6. This ensures our trimming environment is identical to that in the study, supporting strict reproducibility.

Each input FASTQ file is trimmed using a quality threshold of Q=20 (which keeps bases with $\geq99$\% base call accuracy) and sequences are retained only if they are at least 25 bases in length. The step also compresses the output for efficient storage and downstream use. The container setup verifies the installed version of Trim Galore to prevent discrepancies.

Trimming is automated in the workflow by the following Nextflow process:

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
# Remove .gz extension and .fastq extension to get SRA ID
SRA_ID=$(basename ${fastq_files} .fastq.gz)

trim_galore -q 20 --phred33 --length 25 ${SRA_ID}.fastq.gz

# Rename output file
# mv ${SRA_ID}.fastq.gz_trimmed.fq.gz ${SRA_ID}_trimmed.fq.gz
\end{lstlisting}

This script extracts the SRA ID from each filename and processes it with the specified parameters, ensuring that downstream alignment and quantification are performed on high-quality, adapter-free reads. By replicating both the software versions and the precise logic of the original workflow, we maintain analytical fidelity and reliability throughout this step.

\subsection{Reference Genome and Annotation Download}
To replicate the RNA-seq analysis pipeline exactly, the reference genome and its genome annotation for \textit{Staphylococcus aureus} were retrieved from NCBI using the accession number CP000253.1. This step was automated in the reproducible workflow using a dedicated Nextflow process, \texttt{GET\_DATA\_GENOME}, which interfaces with the NCBI Entrez system for nucleotide data retrieval.

The process downloads two essential files: the reference genome sequence in FASTA format (\texttt{reference.fasta}), and the genome annotation in GFF3 format (\texttt{annotation.gff}). Both files serve as critical inputs for mapping and feature quantification steps downstream.

This process uses the \texttt{esearch} and \texttt{efetch} utilities provided by the Entrez Direct Docker container to ensure consistency and reproducibility. The container guarantees that the exact same versions of Entrez tools are used, avoiding variability in database queries.

\textbf{Process script excerpt:}
\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
esearch -db nucleotide -query $ref_genome | efetch -format fasta > reference.fasta
esearch -db nuccore -query $ref_genome | efetch -format gff3 > annotation.gff
\end{lstlisting}

Here, \texttt{\$ref\_genome} corresponds to the NCBI accession CP000253.1. This two-step command first queries the nucleotide database to download the genome FASTA sequence and then queries the nucleotide core database for the corresponding annotation in GFF3 format.

Providing an automated, containerized environment for this step avoids manual downloads, ensures file versions match those used in the original analysis, and supports full reproducibility within the workflow.

\subsection{Index creation using Bowtie}

The next step is to index the reference genome. Without an index, the algorithm would have to linearly scan the entire genome for each read, which would be extremely slow. The index allows the data to be structured for efficient searching.
The library used to perform this indexing is Bowtie. For reproducibility purposes, we work on a docker containing Bowtie version 0.12.7, available at https://sourceforge.net/projects/bowtie-bio/postdownload.
In the following process, we will also use samtools and bowtie. So we create a single “bowtie” docker that contains both libraries with the appropriate versions and the latest ubuntu version.
The docker image is stored on dockerhub at the following address: mariemeier/reprohackathon:bowtie. We create a new nextflow process named \texttt{INDEX\_REF\_GENOME} that takes the reference fastq file as input and returns the indexed reference genome in the results/index directory. Indexing is performed by the bowtie-build function within the docker. The next bash command is executed : 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    mkdir -p index_ref_genome
    bowtie-build ${ref_genome} index_ref_genome/indexed_ref_genome
\end{lstlisting}

The resulting file is an .ebwt file based on the Burrows-Wheeler transform (BWT) of the reference genome. It contains the transformed sequence of the genome, the metadata needed to reconstruct the positions of the patterns in the original genome, and optimized data structures for fast sequence searching.

\subsection{Mapping using Bowtie}

The next process involves mapping the genome being studied to the indexed reference genome. Mapping is aligning short sequences (reads) obtained from sequencing with a reference sequence, which allows us to determine where they are located. The process is executed in the same “bowtie” docker as before. The new nextflow \texttt{MAPPING\_BOWTIE} process generates a .bam file containing the alignments. The process uses 3 CPU cores for execution.
The results are copied to the results/mapping folder, overwriting existing files. 
The process takes as input a tuple containing \texttt{trimmed\_fastq} (path to the compressed fastq file \texttt{*\_trimmed.fq.gz} containing the trimmed reads) and \texttt{indexed\_genome} (path to the folder containing the reference genome index).
The process generates a .bam file for each input FASTQ file. This is a binary (compressed) alignment format that contains the reads aligned to the reference genome. It is created from the .sam file from bowtie using samtools according to the following script: 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    gunzip -c ${trimmed_fastq} | \
    bowtie -S ./index_ref_genome/indexed_ref_genome - | \
    samtools view -@ ${task.cpus} -bS -o ${srr_id}.bam
\end{lstlisting}

\subsection{Feature Count}

This step consists of creating a count matrix from the mapped sequence file. This involves counting the number of reads aligned with each gene between the two conditions. 
We are working on a docker containing SubReads version 1.4.6-p3 available here: https://sourceforge.net/projects/subread/. It requires Ubuntu version 22.04 to use the featureCounts binary. The process named FEATURECOUNTS takes the .bam file of mapped reads as input and the gff file containing the genes location for the reference genome obtained from the process \texttt{GET\_DATA\_GENOME}. The output is a count matrix, where rows represent genes and columns represent samples, and a summary file is generated for each sample, detailing statistics such as the number of assigned and unassigned reads. The next script is executed : 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    featureCounts -t gene -g ID -s 1 
    -a ${gff_file} -o counts.txt ${mapped_reads}
\end{lstlisting}

\subsection{Creation of ColData file}

In order to know which SRR corresponds to which condition, we create a tsv file containing two columns, the first being \texttt{ID\_SRR} and the second being the condition (IP or control).
To do this, the \texttt{GET\_GEO\_TABLE} process downloads and extracts metadata from the NCBI's GEO (Gene Expression Omnibus) database. It generates a table file (TSV) that associates sample identifiers (GSM) with their respective experimental conditions (e.g., “IP” or ‘control’). It takes \texttt{geo\_id} as input and returns the corresponding tsv file. The following script is executed in the “entrez-direct” docker:

\begin{verbatim}
      geo_parent=\$(echo ${geo_id} | sed -E 's/(GSE[0-9]{3})[0-9]
      +/\\1nnn/')
      wget -q ftp://ftp.ncbi.nlm.nih.gov/geo/series/\${geo_parent}
      /${geo_id}/soft/${geo_id}_family.soft.gz -O ${geo_id}.soft.gz
      gunzip -f ${geo_id}.soft.gz
      awk '
      /^\\^SAMPLE/ {gsm=\$3}
      /^!Sample_description/ {
          desc=\$3
          if (desc ~ /^IP/) cond="IP"
          else if (desc ~ /^ctrl/) cond="control"
          else cond="unknown"
          print gsm"\\t"cond
      }' ${geo_id}.soft > ${geo_id}_table.tsv
\end{verbatim}

Furthermore, the \texttt{GET\_SRA\_DATA} process takes an SRP value as input and returns a CSV file containing the metadata for the SRA runs associated with the project. The following script performs this task and is executed in the container “entrez-direct”: 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    esearch -db sra -query "${sra_project}" 
    | efetch -format runinfo > "sra_data_complete.csv"
\end{lstlisting}

Then the \texttt{CREATE\_COLDATA} process uses a R script to merge the GEO and SRA metadata and generate a structured colData file. It takes as input a tuple containing the paths to \texttt{geo\_id}, \texttt{sra\_data\_file}, and the R script, and returns the structured tsv coldata file for further analysis. It is executed in the deseq2 container at the following address: mariemeier/reprohackathon:deseq2. It contains R version 3.4.1, CRAN packages, Biocinstaller 3.5 to obtain the bioconductor packages, Rgraphviz version 2.54.0, and DESeq2 version 1.16.1. The script is : 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    Rscript ${script_R} ${geo_id} ${sra_data_file} coldata.tsv
\end{lstlisting}

\subsection{Statistical Analysis (DESeq2)}

The last process performs statistical analysis in order to reproduce the two genome comparison graphs. Ran in the container deseq2, the nextflow process \texttt{STAT\_ANALYSIS} takes as input a tuple with the paths of coldata, \texttt{count\_matrix}, and the R script and returns the plot according to the following script: 

\begin{lstlisting}[language=bash, frame=single, basicstyle=\ttfamily\small, breaklines=true]
    Rscript ${script_R} ${coldata} ${count_matrix} ma_plot.png
\end{lstlisting}

\section{Results}

\subsection{Count Matrix}

The read-counting step using \texttt{featureCounts} (v1.4.6-p3) produced a gene-level count matrix for the six RNA-seq samples, corresponding to the three persister and three control conditions. The command was run in strand-specific mode (\texttt{-s 1}) on features annotated as genes in \texttt{annotation.gff} (\texttt{-t gene}, \texttt{-g ID}), and took as input the six BAM files generated by the Bowtie mapping step (\texttt{SRR10379725.bam}, \texttt{SRR10379724.bam}, \texttt{SRR10379723.bam}, \texttt{SRR10379722.bam}, \texttt{SRR10379721.bam}, \texttt{SRR10379726.bam}).

The resulting \texttt{counts.txt} file is organized with one header line containing genomic coordinates and one column per sample, followed by one row per gene such as \texttt{gene-SAOUHSC\_00001}, \texttt{gene-SAOUHSC\_00002}, etc. 
For each gene, the matrix reports its chromosome (CP000253.1), start and end positions, strand, length, and the raw integer counts in the six samples, for example very highly expressed genes such as \texttt{gene-SAOUHSC\_00009} (up to tens of thousands of reads) and more weakly expressed genes with only a few reads. This structure matches the expected gene-by-sample input format for downstream normalization and differential expression analysis with DESeq2.

\subsection{Differential Expression and Reproduction of the MA-plot}

Using the \texttt{STAT\_ANALYSIS.R} script, the DESeq2 analysis was executed directly from the count matrix and \texttt{coldata} file to reproduce the MA-plot of the complete RNA-seq dataset shown in Supplementary Figure~3 of Peyrusson \textit{et al.}. The script builds a \texttt{DESeqDataSet} with design \texttt{\textasciitilde{} Condition}, runs the standard DESeq2 pipeline with default parameters, and extracts the contrast \textit{IP vs.\ control}, yielding for each gene a base mean of normalized counts, a log$_2$ fold change, and an adjusted $p$-value. From these results, we computed the MA coordinates (A = log$_2$(baseMean), M=log$_2$ fold change) and generated a custom base‑R plot, exporting the figure as \texttt{ma\_plot.png} (Figure~\ref{fig:ma_plot_ours}).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{ma_plot.png}
    \caption{MA-plot of the complete RNA-seq dataset obtained with our DESeq2 analysis (IP vs.\ control). Red points indicate genes with adjusted $p$-value $< 0.05$, black points non-significant genes.}
    \label{fig:ma_plot_ours}
\end{figure}

Our MA-plot reproduces both the global structure and the visual style of the original supplementary figure: genes are centred around a log$_2$ fold change of zero, with increased dispersion at low counts and a clear band of significant genes (red points) extending towards positive and negative fold changes at higher mean expression values. As in the publication, log$_2$ fold changes are truncated at ±4, with points beyond these thresholds shown as triangles at the top or bottom of the plot, and significance is highlighted with an adjusted $p$-value cutoff of 0.05. Qualitatively, the density and spread of significant genes across the dynamic range of normalized counts are very similar to those reported by Peyrusson \textit{et al.}, indicating that our workflow rapidly recovers a transcriptomic signature that is globally consistent with the original RNA‑seq differential expression analysis.

\subsection{Repro}

\section{Conclusion/Perspectives}

This reprohackathon project presented a great opportunity to confront both the practical and theoretical challenges of reproducing published RNA-seq analyses in a fully transparent and reproducible manner. Among the key lessons learned was the care that needed to be taken in selecting, installing, and documenting the correct versions of all libraries and tools. The creation of Docker containerized environments for each main component of the analyses, such as the SRA Toolkit, Bowtie, and Samtools, ensured computational reproducibility and avoided a series of common problems related to software incompatibilities and version changes.

The time consumed by researching and checking compatible library versions was not superfluous---this was quite important at each step of the workflow execution and for supporting collaboration within the team. Indexing the reference genome and mapping sequence reads to it, using Nextflow workflows and Dockerized Bowtie, showed how crucial accurate sequence alignment is for later gene expression analyses. Thanks to careful workflow management and strict control of the computational environment, the mapping steps produced reliable alignment results, forming a solid basis for differential expression analysis. This reproducible workflow made it possible to correctly identify and extract sequences of interest---specifically, the differentially expressed genes typical of intracellular \textit{S. aureus} persisters exposed to antibiotics.

Looking forward, this project highlights the increasing importance of automation and reproducibility frameworks in computational biology. Future work should aim to further improve workflow modularity and scalability, extend the pipeline to support more experimental situations, and integrate new analytical tools as they become available. Finally, keeping a strong focus on transparency, documentation, and version control will remain essential to ensure that bioinformatics analyses stay robust, shareable, and accessible to the wider scientific community.

\section{References}
\begin{enumerate}
    \item Peyrusson, F., Varet, H., Nguyen, T. K., Legendre, R., Sismeiro, O., Coppe, J. Y., Leduc, D. (2020). Intracellular \textit{Staphylococcus aureus} persisters upon antibiotic exposure. \textit{Nature Communications}, 11, 2200. \url{https://doi.org/10.1038/s41467-020-15966-7}
    \item Langmead, B., Trapnell, C., Pop, M., Salzberg, S. L. (2009). Ultrafast and memory-efficient alignment of short DNA sequences to the human genome. \textit{Genome Biology}, 10(3), R25. \url{https://doi.org/10.1186/gb-2009-10-3-r25}
    \item Love, M. I., Huber, W., Anders, S. (2014). Moderated estimation of fold change and dispersion for RNA-seq data with DESeq2. \textit{Genome Biology}, 15(12), 550. \url{https://doi.org/10.1186/s13059-014-0550-8}
    \item Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., ... Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. \textit{Bioinformatics}, 25(16), 2078–2079. \url{https://doi.org/10.1093/bioinformatics/btp352}
    \item Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., Notredame, C. (2017). Nextflow enables reproducible computational workflows. \textit{Nature Biotechnology}, 35(4), 316–319. \url{https://doi.org/10.1038/nbt.3820}
    \item Merkel, D. (2014). Docker: Lightweight Linux containers for consistent development and deployment. \textit{Linux Journal}, 2014(239), 2.
    \item Ewels, P., Magnusson, M., Lundin, S., Käller, M. (2016). MultiQC: Summarize analysis results for multiple tools and samples in a single report. \textit{Bioinformatics}, 32(19), 3047–3048. \url{https://doi.org/10.1093/bioinformatics/btw354}
    \item Bolger, A. M., Lohse, M., Usadel, B. (2014). Trimmomatic: A flexible trimmer for Illumina sequence data. \textit{Bioinformatics}, 30(15), 2114–2120. \url{https://doi.org/10.1093/bioinformatics/btu170}
    \item Liao, Y., Smyth, G. K., Shi, W. (2014). featureCounts: An efficient general purpose program for assigning sequence reads to genomic features. \textit{Bioinformatics}, 30(7), 923–930. \url{https://doi.org/10.1093/bioinformatics/btt656}
    \item Kodama, Y., Shumway, M., Leinonen, R. (2012). The Sequence Read Archive: Rapid sharing of data and metadata related to sequence submissions. \textit{Nucleic Acids Research}, 40(D1), D54–D56. \url{https://doi.org/10.1093/nar/gkr854}
    \item Git - Version Control System. Available at \url{https://git-scm.com} Accessed November 2025.
    \item Docker Documentation. Available at \url{https://docs.docker.com} Accessed November 2025.
\end{enumerate}


\end{document}